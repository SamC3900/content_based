{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset/data.csv')\n",
    "df['text'] = df['title'] + df['summary']\n",
    "\n",
    "df.drop(columns=['index', 'title', 'summary'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>Drowned Wednesday Drowned Wednesday is the fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>The Lost Hero As the book opens, Jason awakens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>The Eyes of the Overworld Cugel is easily pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>Magic's Promise The book opens with Herald-Mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>Taran Wanderer Taran and Gurgi have returned t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     genre                                               text\n",
       "0  fantasy  Drowned Wednesday Drowned Wednesday is the fir...\n",
       "1  fantasy  The Lost Hero As the book opens, Jason awakens...\n",
       "2  fantasy  The Eyes of the Overworld Cugel is easily pers...\n",
       "3  fantasy  Magic's Promise The book opens with Herald-Mag...\n",
       "4  fantasy  Taran Wanderer Taran and Gurgi have returned t..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4657 entries, 0 to 4656\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   genre   4657 non-null   object\n",
      " 1   text    4657 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 72.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4657, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['fantasy', 'science', 'crime', 'history', 'horror', 'thriller',\n",
       "        'psychology', 'romance', 'sports', 'travel'], dtype=object),\n",
       " (10,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = df['genre'].unique()\n",
    "classes, classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fantasy': 876, 'science': 647, 'crime': 500, 'history': 600, 'horror': 600, 'thriller': 1023, 'psychology': 100, 'romance': 111, 'sports': 100, 'travel': 100}\n"
     ]
    }
   ],
   "source": [
    "classes_count_dct = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['genre'] in classes_count_dct:\n",
    "        classes_count_dct[row['genre']] += 1\n",
    "    else:\n",
    "        classes_count_dct[row['genre']] = 1\n",
    "\n",
    "print(classes_count_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samcolgan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def max_dct_count(dct):\n",
    "    return max(dct.values())\n",
    "\n",
    "def calc_alphas(classes, max_count, count_dct):\n",
    "    return {cls: 1 - count_dct[cls]/max_count for cls in classes}\n",
    "\n",
    "def join_lists(lst):\n",
    "    final = []\n",
    "    for l in lst:\n",
    "        final.extend(l)\n",
    "    return final\n",
    "\n",
    "def word_replacement(genre, text):\n",
    "    final = ''\n",
    "    for word in text.split():\n",
    "        synonyms = join_lists(wordnet.synonyms(word))\n",
    "        \n",
    "        choice = word\n",
    "\n",
    "        if synonyms and len(word) >= 3:\n",
    "            synonyms = synonyms[0]\n",
    "            threshold = 0.7 # 70% of words will be changed\n",
    "            choice = random.choice(synonyms).lower() if random.uniform(0, 1) < threshold else choice\n",
    "\n",
    "        final += choice + ' '\n",
    "\n",
    "    return {'genre': genre, 'text': final.strip()} \n",
    "\n",
    "def data_aug(df, count_dct, classes):\n",
    "    max_count = max_dct_count(count_dct)\n",
    "    alphas_dct = calc_alphas(classes, max_count, count_dct)\n",
    "    total_added = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        cls = row['genre']\n",
    "\n",
    "        while alphas_dct[cls] > random.uniform(0, 1) and count_dct[cls] < max_count:\n",
    "            df = df._append(word_replacement(cls, row['text']), ignore_index=True)\n",
    "            total_added += 1\n",
    "            count_dct[cls] += 1\n",
    "\n",
    "        if index % 10 == 0:\n",
    "            print(f'{index} samples augmented, {total_added} new samples added')\n",
    "\n",
    "    print('done')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples augmented, 0 new samples added\n",
      "10 samples augmented, 1 new samples added\n",
      "20 samples augmented, 2 new samples added\n",
      "30 samples augmented, 4 new samples added\n",
      "40 samples augmented, 4 new samples added\n",
      "50 samples augmented, 7 new samples added\n",
      "60 samples augmented, 8 new samples added\n",
      "70 samples augmented, 8 new samples added\n",
      "80 samples augmented, 9 new samples added\n",
      "90 samples augmented, 14 new samples added\n",
      "100 samples augmented, 16 new samples added\n",
      "110 samples augmented, 17 new samples added\n",
      "120 samples augmented, 17 new samples added\n",
      "130 samples augmented, 18 new samples added\n",
      "140 samples augmented, 21 new samples added\n",
      "150 samples augmented, 23 new samples added\n",
      "160 samples augmented, 24 new samples added\n",
      "170 samples augmented, 26 new samples added\n",
      "180 samples augmented, 27 new samples added\n",
      "190 samples augmented, 30 new samples added\n",
      "200 samples augmented, 30 new samples added\n",
      "210 samples augmented, 30 new samples added\n",
      "220 samples augmented, 33 new samples added\n",
      "230 samples augmented, 34 new samples added\n",
      "240 samples augmented, 36 new samples added\n",
      "250 samples augmented, 42 new samples added\n",
      "260 samples augmented, 44 new samples added\n",
      "270 samples augmented, 45 new samples added\n",
      "280 samples augmented, 50 new samples added\n",
      "290 samples augmented, 50 new samples added\n",
      "300 samples augmented, 50 new samples added\n",
      "310 samples augmented, 51 new samples added\n",
      "320 samples augmented, 54 new samples added\n",
      "330 samples augmented, 55 new samples added\n",
      "340 samples augmented, 57 new samples added\n",
      "350 samples augmented, 60 new samples added\n",
      "360 samples augmented, 62 new samples added\n",
      "370 samples augmented, 64 new samples added\n",
      "380 samples augmented, 67 new samples added\n",
      "390 samples augmented, 67 new samples added\n",
      "400 samples augmented, 69 new samples added\n",
      "410 samples augmented, 69 new samples added\n",
      "420 samples augmented, 70 new samples added\n",
      "430 samples augmented, 70 new samples added\n",
      "440 samples augmented, 71 new samples added\n",
      "450 samples augmented, 72 new samples added\n",
      "460 samples augmented, 72 new samples added\n",
      "470 samples augmented, 74 new samples added\n",
      "480 samples augmented, 75 new samples added\n",
      "490 samples augmented, 77 new samples added\n",
      "500 samples augmented, 77 new samples added\n",
      "510 samples augmented, 83 new samples added\n",
      "520 samples augmented, 85 new samples added\n",
      "530 samples augmented, 91 new samples added\n",
      "540 samples augmented, 98 new samples added\n",
      "550 samples augmented, 101 new samples added\n",
      "560 samples augmented, 108 new samples added\n",
      "570 samples augmented, 116 new samples added\n",
      "580 samples augmented, 119 new samples added\n",
      "590 samples augmented, 129 new samples added\n",
      "600 samples augmented, 135 new samples added\n",
      "610 samples augmented, 139 new samples added\n",
      "620 samples augmented, 143 new samples added\n",
      "630 samples augmented, 148 new samples added\n",
      "640 samples augmented, 157 new samples added\n",
      "650 samples augmented, 163 new samples added\n",
      "660 samples augmented, 167 new samples added\n",
      "670 samples augmented, 169 new samples added\n",
      "680 samples augmented, 172 new samples added\n",
      "690 samples augmented, 181 new samples added\n",
      "700 samples augmented, 182 new samples added\n",
      "710 samples augmented, 192 new samples added\n",
      "720 samples augmented, 197 new samples added\n",
      "730 samples augmented, 203 new samples added\n",
      "740 samples augmented, 206 new samples added\n",
      "750 samples augmented, 209 new samples added\n",
      "760 samples augmented, 214 new samples added\n",
      "770 samples augmented, 220 new samples added\n",
      "780 samples augmented, 224 new samples added\n",
      "790 samples augmented, 228 new samples added\n",
      "800 samples augmented, 234 new samples added\n",
      "810 samples augmented, 236 new samples added\n",
      "820 samples augmented, 239 new samples added\n",
      "830 samples augmented, 244 new samples added\n",
      "840 samples augmented, 245 new samples added\n",
      "850 samples augmented, 250 new samples added\n",
      "860 samples augmented, 255 new samples added\n",
      "870 samples augmented, 258 new samples added\n",
      "880 samples augmented, 265 new samples added\n",
      "890 samples augmented, 272 new samples added\n",
      "900 samples augmented, 278 new samples added\n",
      "910 samples augmented, 286 new samples added\n",
      "920 samples augmented, 293 new samples added\n",
      "930 samples augmented, 301 new samples added\n",
      "940 samples augmented, 304 new samples added\n",
      "950 samples augmented, 312 new samples added\n",
      "960 samples augmented, 318 new samples added\n",
      "970 samples augmented, 325 new samples added\n",
      "980 samples augmented, 330 new samples added\n",
      "990 samples augmented, 337 new samples added\n",
      "1000 samples augmented, 340 new samples added\n",
      "1010 samples augmented, 348 new samples added\n",
      "1020 samples augmented, 359 new samples added\n",
      "1030 samples augmented, 375 new samples added\n",
      "1040 samples augmented, 379 new samples added\n",
      "1050 samples augmented, 390 new samples added\n",
      "1060 samples augmented, 403 new samples added\n",
      "1070 samples augmented, 413 new samples added\n",
      "1080 samples augmented, 422 new samples added\n",
      "1090 samples augmented, 432 new samples added\n",
      "1100 samples augmented, 438 new samples added\n",
      "1110 samples augmented, 447 new samples added\n",
      "1120 samples augmented, 457 new samples added\n",
      "1130 samples augmented, 464 new samples added\n",
      "1140 samples augmented, 470 new samples added\n",
      "1150 samples augmented, 481 new samples added\n",
      "1160 samples augmented, 493 new samples added\n",
      "1170 samples augmented, 505 new samples added\n",
      "1180 samples augmented, 522 new samples added\n",
      "1190 samples augmented, 540 new samples added\n",
      "1200 samples augmented, 549 new samples added\n",
      "1210 samples augmented, 564 new samples added\n",
      "1220 samples augmented, 577 new samples added\n",
      "1230 samples augmented, 593 new samples added\n",
      "1240 samples augmented, 604 new samples added\n",
      "1250 samples augmented, 618 new samples added\n",
      "1260 samples augmented, 627 new samples added\n",
      "1270 samples augmented, 634 new samples added\n",
      "1280 samples augmented, 640 new samples added\n",
      "1290 samples augmented, 657 new samples added\n",
      "1300 samples augmented, 667 new samples added\n",
      "1310 samples augmented, 671 new samples added\n",
      "1320 samples augmented, 683 new samples added\n",
      "1330 samples augmented, 690 new samples added\n",
      "1340 samples augmented, 701 new samples added\n",
      "1350 samples augmented, 718 new samples added\n",
      "1360 samples augmented, 730 new samples added\n",
      "1370 samples augmented, 739 new samples added\n",
      "1380 samples augmented, 751 new samples added\n",
      "1390 samples augmented, 766 new samples added\n",
      "1400 samples augmented, 774 new samples added\n",
      "1410 samples augmented, 779 new samples added\n",
      "1420 samples augmented, 786 new samples added\n",
      "1430 samples augmented, 796 new samples added\n",
      "1440 samples augmented, 808 new samples added\n",
      "1450 samples augmented, 814 new samples added\n",
      "1460 samples augmented, 822 new samples added\n",
      "1470 samples augmented, 830 new samples added\n",
      "1480 samples augmented, 839 new samples added\n",
      "1490 samples augmented, 844 new samples added\n",
      "1500 samples augmented, 850 new samples added\n",
      "1510 samples augmented, 853 new samples added\n",
      "1520 samples augmented, 867 new samples added\n",
      "1530 samples augmented, 877 new samples added\n",
      "1540 samples augmented, 889 new samples added\n",
      "1550 samples augmented, 891 new samples added\n",
      "1560 samples augmented, 895 new samples added\n",
      "1570 samples augmented, 905 new samples added\n",
      "1580 samples augmented, 908 new samples added\n",
      "1590 samples augmented, 916 new samples added\n",
      "1600 samples augmented, 920 new samples added\n",
      "1610 samples augmented, 922 new samples added\n",
      "1620 samples augmented, 930 new samples added\n",
      "1630 samples augmented, 935 new samples added\n",
      "1640 samples augmented, 945 new samples added\n",
      "1650 samples augmented, 955 new samples added\n",
      "1660 samples augmented, 963 new samples added\n",
      "1670 samples augmented, 968 new samples added\n",
      "1680 samples augmented, 975 new samples added\n",
      "1690 samples augmented, 978 new samples added\n",
      "1700 samples augmented, 986 new samples added\n",
      "1710 samples augmented, 996 new samples added\n",
      "1720 samples augmented, 1003 new samples added\n",
      "1730 samples augmented, 1005 new samples added\n",
      "1740 samples augmented, 1014 new samples added\n",
      "1750 samples augmented, 1028 new samples added\n",
      "1760 samples augmented, 1030 new samples added\n",
      "1770 samples augmented, 1031 new samples added\n",
      "1780 samples augmented, 1043 new samples added\n",
      "1790 samples augmented, 1047 new samples added\n",
      "1800 samples augmented, 1051 new samples added\n",
      "1810 samples augmented, 1059 new samples added\n",
      "1820 samples augmented, 1061 new samples added\n",
      "1830 samples augmented, 1065 new samples added\n",
      "1840 samples augmented, 1073 new samples added\n",
      "1850 samples augmented, 1076 new samples added\n",
      "1860 samples augmented, 1087 new samples added\n",
      "1870 samples augmented, 1099 new samples added\n",
      "1880 samples augmented, 1106 new samples added\n",
      "1890 samples augmented, 1114 new samples added\n",
      "1900 samples augmented, 1121 new samples added\n",
      "1910 samples augmented, 1126 new samples added\n",
      "1920 samples augmented, 1131 new samples added\n",
      "1930 samples augmented, 1137 new samples added\n",
      "1940 samples augmented, 1144 new samples added\n",
      "1950 samples augmented, 1155 new samples added\n",
      "1960 samples augmented, 1159 new samples added\n",
      "1970 samples augmented, 1168 new samples added\n",
      "1980 samples augmented, 1173 new samples added\n",
      "1990 samples augmented, 1183 new samples added\n",
      "2000 samples augmented, 1188 new samples added\n",
      "2010 samples augmented, 1199 new samples added\n",
      "2020 samples augmented, 1202 new samples added\n",
      "2030 samples augmented, 1212 new samples added\n",
      "2040 samples augmented, 1221 new samples added\n",
      "2050 samples augmented, 1224 new samples added\n",
      "2060 samples augmented, 1230 new samples added\n",
      "2070 samples augmented, 1233 new samples added\n",
      "2080 samples augmented, 1245 new samples added\n",
      "2090 samples augmented, 1255 new samples added\n",
      "2100 samples augmented, 1260 new samples added\n",
      "2110 samples augmented, 1263 new samples added\n",
      "2120 samples augmented, 1269 new samples added\n",
      "2130 samples augmented, 1274 new samples added\n",
      "2140 samples augmented, 1285 new samples added\n",
      "2150 samples augmented, 1291 new samples added\n",
      "2160 samples augmented, 1293 new samples added\n",
      "2170 samples augmented, 1298 new samples added\n",
      "2180 samples augmented, 1303 new samples added\n",
      "2190 samples augmented, 1315 new samples added\n",
      "2200 samples augmented, 1325 new samples added\n",
      "2210 samples augmented, 1336 new samples added\n",
      "2220 samples augmented, 1340 new samples added\n",
      "2230 samples augmented, 1347 new samples added\n",
      "2240 samples augmented, 1355 new samples added\n",
      "2250 samples augmented, 1362 new samples added\n",
      "2260 samples augmented, 1369 new samples added\n",
      "2270 samples augmented, 1374 new samples added\n",
      "2280 samples augmented, 1381 new samples added\n",
      "2290 samples augmented, 1389 new samples added\n",
      "2300 samples augmented, 1393 new samples added\n",
      "2310 samples augmented, 1406 new samples added\n",
      "2320 samples augmented, 1415 new samples added\n",
      "2330 samples augmented, 1428 new samples added\n",
      "2340 samples augmented, 1433 new samples added\n",
      "2350 samples augmented, 1436 new samples added\n",
      "2360 samples augmented, 1438 new samples added\n",
      "2370 samples augmented, 1443 new samples added\n",
      "2380 samples augmented, 1451 new samples added\n",
      "2390 samples augmented, 1451 new samples added\n",
      "2400 samples augmented, 1466 new samples added\n",
      "2410 samples augmented, 1473 new samples added\n",
      "2420 samples augmented, 1481 new samples added\n",
      "2430 samples augmented, 1482 new samples added\n",
      "2440 samples augmented, 1497 new samples added\n",
      "2450 samples augmented, 1504 new samples added\n",
      "2460 samples augmented, 1508 new samples added\n",
      "2470 samples augmented, 1518 new samples added\n",
      "2480 samples augmented, 1525 new samples added\n",
      "2490 samples augmented, 1535 new samples added\n",
      "2500 samples augmented, 1538 new samples added\n",
      "2510 samples augmented, 1538 new samples added\n",
      "2520 samples augmented, 1538 new samples added\n",
      "2530 samples augmented, 1538 new samples added\n",
      "2540 samples augmented, 1538 new samples added\n",
      "2550 samples augmented, 1538 new samples added\n",
      "2560 samples augmented, 1538 new samples added\n",
      "2570 samples augmented, 1538 new samples added\n",
      "2580 samples augmented, 1538 new samples added\n",
      "2590 samples augmented, 1538 new samples added\n",
      "2600 samples augmented, 1538 new samples added\n",
      "2610 samples augmented, 1538 new samples added\n",
      "2620 samples augmented, 1538 new samples added\n",
      "2630 samples augmented, 1538 new samples added\n",
      "2640 samples augmented, 1538 new samples added\n",
      "2650 samples augmented, 1538 new samples added\n",
      "2660 samples augmented, 1538 new samples added\n",
      "2670 samples augmented, 1538 new samples added\n",
      "2680 samples augmented, 1538 new samples added\n",
      "2690 samples augmented, 1538 new samples added\n",
      "2700 samples augmented, 1538 new samples added\n",
      "2710 samples augmented, 1538 new samples added\n",
      "2720 samples augmented, 1538 new samples added\n",
      "2730 samples augmented, 1538 new samples added\n",
      "2740 samples augmented, 1538 new samples added\n",
      "2750 samples augmented, 1538 new samples added\n",
      "2760 samples augmented, 1538 new samples added\n",
      "2770 samples augmented, 1538 new samples added\n",
      "2780 samples augmented, 1538 new samples added\n",
      "2790 samples augmented, 1538 new samples added\n",
      "2800 samples augmented, 1538 new samples added\n",
      "2810 samples augmented, 1538 new samples added\n",
      "2820 samples augmented, 1538 new samples added\n",
      "2830 samples augmented, 1538 new samples added\n",
      "2840 samples augmented, 1538 new samples added\n",
      "2850 samples augmented, 1538 new samples added\n",
      "2860 samples augmented, 1538 new samples added\n",
      "2870 samples augmented, 1538 new samples added\n",
      "2880 samples augmented, 1538 new samples added\n",
      "2890 samples augmented, 1538 new samples added\n",
      "2900 samples augmented, 1538 new samples added\n",
      "2910 samples augmented, 1538 new samples added\n",
      "2920 samples augmented, 1538 new samples added\n",
      "2930 samples augmented, 1538 new samples added\n",
      "2940 samples augmented, 1538 new samples added\n",
      "2950 samples augmented, 1538 new samples added\n",
      "2960 samples augmented, 1538 new samples added\n",
      "2970 samples augmented, 1538 new samples added\n",
      "2980 samples augmented, 1538 new samples added\n",
      "2990 samples augmented, 1538 new samples added\n",
      "3000 samples augmented, 1538 new samples added\n",
      "3010 samples augmented, 1545 new samples added\n",
      "3020 samples augmented, 1549 new samples added\n",
      "3030 samples augmented, 1560 new samples added\n",
      "3040 samples augmented, 1570 new samples added\n",
      "3050 samples augmented, 1579 new samples added\n",
      "3060 samples augmented, 1586 new samples added\n",
      "3070 samples augmented, 1591 new samples added\n",
      "3080 samples augmented, 1595 new samples added\n",
      "3090 samples augmented, 1602 new samples added\n",
      "3100 samples augmented, 1606 new samples added\n",
      "3110 samples augmented, 1610 new samples added\n",
      "3120 samples augmented, 1613 new samples added\n",
      "3130 samples augmented, 1619 new samples added\n",
      "3140 samples augmented, 1633 new samples added\n",
      "3150 samples augmented, 1643 new samples added\n",
      "3160 samples augmented, 1650 new samples added\n",
      "3170 samples augmented, 1656 new samples added\n",
      "3180 samples augmented, 1662 new samples added\n",
      "3190 samples augmented, 1668 new samples added\n",
      "3200 samples augmented, 1677 new samples added\n",
      "3210 samples augmented, 1773 new samples added\n",
      "3220 samples augmented, 1844 new samples added\n",
      "3230 samples augmented, 1954 new samples added\n",
      "3240 samples augmented, 2035 new samples added\n",
      "3250 samples augmented, 2143 new samples added\n",
      "3260 samples augmented, 2189 new samples added\n",
      "3270 samples augmented, 2305 new samples added\n",
      "3280 samples augmented, 2387 new samples added\n",
      "3290 samples augmented, 2446 new samples added\n",
      "3300 samples augmented, 2543 new samples added\n",
      "3310 samples augmented, 2607 new samples added\n",
      "3320 samples augmented, 2701 new samples added\n",
      "3330 samples augmented, 2767 new samples added\n",
      "3340 samples augmented, 2819 new samples added\n",
      "3350 samples augmented, 2894 new samples added\n",
      "3360 samples augmented, 3006 new samples added\n",
      "3370 samples augmented, 3075 new samples added\n",
      "3380 samples augmented, 3168 new samples added\n",
      "3390 samples augmented, 3249 new samples added\n",
      "3400 samples augmented, 3302 new samples added\n",
      "3410 samples augmented, 3397 new samples added\n",
      "3420 samples augmented, 3400 new samples added\n",
      "3430 samples augmented, 3406 new samples added\n",
      "3440 samples augmented, 3408 new samples added\n",
      "3450 samples augmented, 3415 new samples added\n",
      "3460 samples augmented, 3418 new samples added\n",
      "3470 samples augmented, 3426 new samples added\n",
      "3480 samples augmented, 3433 new samples added\n",
      "3490 samples augmented, 3434 new samples added\n",
      "3500 samples augmented, 3442 new samples added\n",
      "3510 samples augmented, 3449 new samples added\n",
      "3520 samples augmented, 3456 new samples added\n",
      "3530 samples augmented, 3462 new samples added\n",
      "3540 samples augmented, 3467 new samples added\n",
      "3550 samples augmented, 3469 new samples added\n",
      "3560 samples augmented, 3514 new samples added\n",
      "3570 samples augmented, 3607 new samples added\n",
      "3580 samples augmented, 3682 new samples added\n",
      "3590 samples augmented, 3812 new samples added\n",
      "3600 samples augmented, 3944 new samples added\n",
      "3610 samples augmented, 3996 new samples added\n",
      "3620 samples augmented, 4079 new samples added\n",
      "3630 samples augmented, 4179 new samples added\n",
      "3640 samples augmented, 4291 new samples added\n",
      "3650 samples augmented, 4397 new samples added\n",
      "3660 samples augmented, 4397 new samples added\n",
      "3670 samples augmented, 4397 new samples added\n",
      "3680 samples augmented, 4397 new samples added\n",
      "3690 samples augmented, 4397 new samples added\n",
      "3700 samples augmented, 4397 new samples added\n",
      "3710 samples augmented, 4397 new samples added\n",
      "3720 samples augmented, 4397 new samples added\n",
      "3730 samples augmented, 4397 new samples added\n",
      "3740 samples augmented, 4397 new samples added\n",
      "3750 samples augmented, 4397 new samples added\n",
      "3760 samples augmented, 4397 new samples added\n",
      "3770 samples augmented, 4397 new samples added\n",
      "3780 samples augmented, 4397 new samples added\n",
      "3790 samples augmented, 4397 new samples added\n",
      "3800 samples augmented, 4397 new samples added\n",
      "3810 samples augmented, 4397 new samples added\n",
      "3820 samples augmented, 4397 new samples added\n",
      "3830 samples augmented, 4397 new samples added\n",
      "3840 samples augmented, 4397 new samples added\n",
      "3850 samples augmented, 4397 new samples added\n",
      "3860 samples augmented, 4397 new samples added\n",
      "3870 samples augmented, 4397 new samples added\n",
      "3880 samples augmented, 4397 new samples added\n",
      "3890 samples augmented, 4397 new samples added\n",
      "3900 samples augmented, 4397 new samples added\n",
      "3910 samples augmented, 4397 new samples added\n",
      "3920 samples augmented, 4397 new samples added\n",
      "3930 samples augmented, 4397 new samples added\n",
      "3940 samples augmented, 4397 new samples added\n",
      "3950 samples augmented, 4397 new samples added\n",
      "3960 samples augmented, 4397 new samples added\n",
      "3970 samples augmented, 4397 new samples added\n",
      "3980 samples augmented, 4397 new samples added\n",
      "3990 samples augmented, 4397 new samples added\n",
      "4000 samples augmented, 4397 new samples added\n",
      "4010 samples augmented, 4397 new samples added\n",
      "4020 samples augmented, 4397 new samples added\n",
      "4030 samples augmented, 4397 new samples added\n",
      "4040 samples augmented, 4397 new samples added\n",
      "4050 samples augmented, 4397 new samples added\n",
      "4060 samples augmented, 4397 new samples added\n",
      "4070 samples augmented, 4397 new samples added\n",
      "4080 samples augmented, 4397 new samples added\n",
      "4090 samples augmented, 4397 new samples added\n",
      "4100 samples augmented, 4397 new samples added\n",
      "4110 samples augmented, 4397 new samples added\n",
      "4120 samples augmented, 4397 new samples added\n",
      "4130 samples augmented, 4397 new samples added\n",
      "4140 samples augmented, 4397 new samples added\n",
      "4150 samples augmented, 4397 new samples added\n",
      "4160 samples augmented, 4397 new samples added\n",
      "4170 samples augmented, 4397 new samples added\n",
      "4180 samples augmented, 4397 new samples added\n",
      "4190 samples augmented, 4491 new samples added\n",
      "4200 samples augmented, 4577 new samples added\n",
      "4210 samples augmented, 4673 new samples added\n",
      "4220 samples augmented, 4741 new samples added\n",
      "4230 samples augmented, 4839 new samples added\n",
      "4240 samples augmented, 4933 new samples added\n",
      "4250 samples augmented, 5045 new samples added\n",
      "4260 samples augmented, 5145 new samples added\n",
      "4270 samples augmented, 5266 new samples added\n",
      "4280 samples augmented, 5320 new samples added\n",
      "4290 samples augmented, 5320 new samples added\n",
      "4300 samples augmented, 5322 new samples added\n",
      "4310 samples augmented, 5324 new samples added\n",
      "4320 samples augmented, 5325 new samples added\n",
      "4330 samples augmented, 5325 new samples added\n",
      "4340 samples augmented, 5328 new samples added\n",
      "4350 samples augmented, 5329 new samples added\n",
      "4360 samples augmented, 5330 new samples added\n",
      "4370 samples augmented, 5332 new samples added\n",
      "4380 samples augmented, 5332 new samples added\n",
      "4390 samples augmented, 5334 new samples added\n",
      "4400 samples augmented, 5334 new samples added\n",
      "4410 samples augmented, 5334 new samples added\n",
      "4420 samples augmented, 5337 new samples added\n",
      "4430 samples augmented, 5337 new samples added\n",
      "4440 samples augmented, 5340 new samples added\n",
      "4450 samples augmented, 5342 new samples added\n",
      "4460 samples augmented, 5344 new samples added\n",
      "4470 samples augmented, 5344 new samples added\n",
      "4480 samples augmented, 5348 new samples added\n",
      "4490 samples augmented, 5350 new samples added\n",
      "4500 samples augmented, 5350 new samples added\n",
      "4510 samples augmented, 5353 new samples added\n",
      "4520 samples augmented, 5354 new samples added\n",
      "4530 samples augmented, 5354 new samples added\n",
      "4540 samples augmented, 5354 new samples added\n",
      "4550 samples augmented, 5356 new samples added\n",
      "4560 samples augmented, 5356 new samples added\n",
      "4570 samples augmented, 5358 new samples added\n",
      "4580 samples augmented, 5359 new samples added\n",
      "4590 samples augmented, 5360 new samples added\n",
      "4600 samples augmented, 5362 new samples added\n",
      "4610 samples augmented, 5368 new samples added\n",
      "4620 samples augmented, 5371 new samples added\n",
      "4630 samples augmented, 5375 new samples added\n",
      "4640 samples augmented, 5377 new samples added\n",
      "4650 samples augmented, 5378 new samples added\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "df = data_aug(df, classes_count_dct, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fantasy': 1014, 'science': 987, 'crime': 1010, 'history': 1006, 'horror': 1018, 'thriller': 1023, 'psychology': 968, 'romance': 966, 'sports': 1023, 'travel': 1023}\n"
     ]
    }
   ],
   "source": [
    "classes_count_dct = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['genre'] in classes_count_dct:\n",
    "        classes_count_dct[row['genre']] += 1\n",
    "    else:\n",
    "        classes_count_dct[row['genre']] = 1\n",
    "\n",
    "print(classes_count_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(df['text']).toarray()\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8030, 10000), (2008, 10000), (8030,), (2008,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(df['genre'])\n",
    "\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', mode='auto', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                320032    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 32)                128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320490 (1.22 MB)\n",
      "Trainable params: 320426 (1.22 MB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(classes.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2713 - accuracy: 0.6310 - val_loss: 2.1071 - val_accuracy: 0.8436 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4018 - accuracy: 0.9193 - val_loss: 1.9451 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9659 - val_loss: 1.7422 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.9778 - val_loss: 1.4754 - val_accuracy: 0.8924 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9831 - val_loss: 1.1626 - val_accuracy: 0.8894 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9842 - val_loss: 0.8374 - val_accuracy: 0.9004 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9853 - val_loss: 0.5687 - val_accuracy: 0.9004 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9822 - val_loss: 0.4200 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0532 - accuracy: 0.9858\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9853 - val_loss: 0.3554 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0477 - accuracy: 0.9872 - val_loss: 0.3514 - val_accuracy: 0.8979 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0420 - accuracy: 0.9882 - val_loss: 0.3488 - val_accuracy: 0.8989 - lr: 5.0000e-04\n",
      "63/63 [==============================] - 0s 764us/step - loss: 0.8374 - accuracy: 0.9004\n",
      " 1/63 [..............................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samcolgan/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 807us/step\n",
      "[0.8374140858650208, 0.9003984332084656]\n"
     ]
    }
   ],
   "source": [
    "output = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                   epochs=epochs, batch_size=batch_size,\n",
    "                   callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "path = './model.h5'\n",
    "model.save(path)\n",
    "\n",
    "eval = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
