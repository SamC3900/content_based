{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Content Based Recommender"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we load in the books data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OP_hPmjeTzMw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Id                                               Name  \\\n",
            "0  1900511                                         Barbarossa   \n",
            "1  1900512  Collector's Guide to German World War II: Comb...   \n",
            "2  1900514                               Images of Barbarossa   \n",
            "3  1900520        Romania After 2000: Five New Romanian Plays   \n",
            "4  1900521           Global Foreigners: An Anthology of Plays   \n",
            "\n",
            "                  Authors        ISBN  Rating  PublishYear  PublishMonth  \\\n",
            "0      Christopher Ailsby  1840138009     3.0         2007             4   \n",
            "1      Christopher Ailsby  0781802253     0.0         1994             7   \n",
            "2      Christopher Ailsby  0711028257     3.5         2001             1   \n",
            "3  Daniel Charles Gerould  0595436560     4.0         2007             9   \n",
            "4        Saviana Stănescu  1905422423     4.6         2006            12   \n",
            "\n",
            "   PublishDay                                    Publisher RatingDist5  ...  \\\n",
            "0           1                               New Line Books         5:0  ...   \n",
            "1           1                             Hippocrene Books         5:0  ...   \n",
            "2          25                                Ian Allan Ltd         5:0  ...   \n",
            "3           1  Martin E. Segal Theatre Center Publications         5:1  ...   \n",
            "4           7                                Seagull Books         5:4  ...   \n",
            "\n",
            "  RatingDist3 RatingDist2 RatingDist1 RatingDistTotal CountsOfReview  \\\n",
            "0         3:1         2:0         1:0         total:1              0   \n",
            "1         3:0         2:0         1:0         total:0              0   \n",
            "2         3:2         2:1         1:0         total:8              0   \n",
            "3         3:1         2:0         1:0         total:6              0   \n",
            "4         3:1         2:0         1:0         total:5              0   \n",
            "\n",
            "   Language PagesNumber                                        Description  \\\n",
            "0       NaN       192.0  On 22 June 1941, Adolf Hitler launched Operati...   \n",
            "1       NaN       160.0                                                NaN   \n",
            "2       NaN       256.0  On 22 June 1941, Adolf Hitler launched Operati...   \n",
            "3       NaN       226.0  The first anthology of new Romanian Drama publ...   \n",
            "4       NaN       320.0  In Waxing West, Daniella, newly arrived in the...   \n",
            "\n",
            "  pagesNumber  Count of text reviews  \n",
            "0         NaN                    NaN  \n",
            "1         NaN                    NaN  \n",
            "2         NaN                    NaN  \n",
            "3         NaN                    NaN  \n",
            "4         NaN                    NaN  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "# Loading the Data\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "directory = './dataset'\n",
        "csv_files = glob.glob(os.path.join(directory, 'book*.csv'))\n",
        "dataframes = [pd.read_csv(file) for file in csv_files]\n",
        "book_data = pd.concat(dataframes, ignore_index=True)\n",
        "print(book_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then load in the ratings data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     ID                                   Name Rating\n",
            "580   5                      The Secret Garden      5\n",
            "581   5         Remarkable Women of California      5\n",
            "582   5                       The Forsyte Saga      5\n",
            "583   5  The Crystal Cave (Arthurian Saga, #1)      4\n",
            "584   5      Me Before You (Me Before You, #1)      5\n",
            "(342591, 3)\n",
            "Number of unique users with ratings: 4118\n",
            "Number of unique books in ratings data: 102744\n"
          ]
        }
      ],
      "source": [
        "rating_df = pd.DataFrame()\n",
        "for i, file in enumerate(os.listdir(directory)):\n",
        "    if re.match(r'user_rating_*',file):\n",
        "        data = pd.read_csv(directory+'/'+file)\n",
        "        rating_df = pd.concat([rating_df, data], axis=0, join='outer')\n",
        "\n",
        "# Delete any rows that refer to a users with no ratings\n",
        "rating_df = rating_df.drop(rating_df[rating_df['Rating'] == \"This user doesn't have any rating\"].index)\n",
        "\n",
        "# Replace textual ratings with numerical ratings\n",
        "ratings = {'did not like it': 1, 'it was ok': 2, 'liked it': 3, 'really liked it': 4, 'it was amazing': 5}\n",
        "for textual, numerical in ratings.items():\n",
        "    rating_df['Rating'] = rating_df['Rating'].mask(rating_df['Rating'] == textual, numerical)\n",
        "\n",
        "# Visualise the data and its shape\n",
        "print(rating_df.head())\n",
        "print(rating_df.shape)\n",
        "\n",
        "# Print the number of unique user ID's and book names in the dataframe\n",
        "print(f\"Number of unique users with ratings: {rating_df['ID'].nunique()}\")\n",
        "print(f\"Number of unique books in ratings data: {rating_df['Name'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3kzfaBe6OmZ"
      },
      "source": [
        "There are missing values in our books data so we remove the rows with missing publisher and description since we need these features. We merge the identical features pagesNumber and PagesNumber. We then combine Name, Authors, PublishYear, Publisher, Description into one column named Content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PkHBgGVl6XK7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Id                             0\n",
            "Name                           0\n",
            "Authors                        0\n",
            "ISBN                        5922\n",
            "Rating                         0\n",
            "PublishYear                    0\n",
            "PublishMonth                   0\n",
            "PublishDay                     0\n",
            "Publisher                  17823\n",
            "RatingDist5                    0\n",
            "RatingDist4                    0\n",
            "RatingDist3                    0\n",
            "RatingDist2                    0\n",
            "RatingDist1                    0\n",
            "RatingDistTotal                0\n",
            "CountsOfReview                 0\n",
            "Language                 1598399\n",
            "PagesNumber               834966\n",
            "Description               679010\n",
            "pagesNumber              1015232\n",
            "Count of text reviews    1440501\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Name</th>\n",
              "      <th>Authors</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Rating</th>\n",
              "      <th>PublishYear</th>\n",
              "      <th>PublishMonth</th>\n",
              "      <th>PublishDay</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>RatingDist5</th>\n",
              "      <th>...</th>\n",
              "      <th>RatingDist3</th>\n",
              "      <th>RatingDist2</th>\n",
              "      <th>RatingDist1</th>\n",
              "      <th>RatingDistTotal</th>\n",
              "      <th>CountsOfReview</th>\n",
              "      <th>Language</th>\n",
              "      <th>PagesNumber</th>\n",
              "      <th>Description</th>\n",
              "      <th>Count of text reviews</th>\n",
              "      <th>Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1900511</td>\n",
              "      <td>Barbarossa</td>\n",
              "      <td>Christopher Ailsby</td>\n",
              "      <td>1840138009</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2007</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>New Line Books</td>\n",
              "      <td>5:0</td>\n",
              "      <td>...</td>\n",
              "      <td>3:1</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>192.0</td>\n",
              "      <td>On 22 June 1941, Adolf Hitler launched Operati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Barbarossa Christopher Ailsby 2007 New Line Bo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1900514</td>\n",
              "      <td>Images of Barbarossa</td>\n",
              "      <td>Christopher Ailsby</td>\n",
              "      <td>0711028257</td>\n",
              "      <td>3.50</td>\n",
              "      <td>2001</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>Ian Allan Ltd</td>\n",
              "      <td>5:0</td>\n",
              "      <td>...</td>\n",
              "      <td>3:2</td>\n",
              "      <td>2:1</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:8</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>256.0</td>\n",
              "      <td>On 22 June 1941, Adolf Hitler launched Operati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Images of Barbarossa Christopher Ailsby 2001 I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1900520</td>\n",
              "      <td>Romania After 2000: Five New Romanian Plays</td>\n",
              "      <td>Daniel Charles Gerould</td>\n",
              "      <td>0595436560</td>\n",
              "      <td>4.00</td>\n",
              "      <td>2007</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>Martin E. Segal Theatre Center Publications</td>\n",
              "      <td>5:1</td>\n",
              "      <td>...</td>\n",
              "      <td>3:1</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:6</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>226.0</td>\n",
              "      <td>The first anthology of new Romanian Drama publ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Romania After 2000: Five New Romanian Plays Da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1900521</td>\n",
              "      <td>Global Foreigners: An Anthology of Plays</td>\n",
              "      <td>Saviana Stănescu</td>\n",
              "      <td>1905422423</td>\n",
              "      <td>4.60</td>\n",
              "      <td>2006</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>Seagull Books</td>\n",
              "      <td>5:4</td>\n",
              "      <td>...</td>\n",
              "      <td>3:1</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>320.0</td>\n",
              "      <td>In Waxing West, Daniella, newly arrived in the...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Global Foreigners: An Anthology of Plays Savia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1900525</td>\n",
              "      <td>Diary of a Clone</td>\n",
              "      <td>Saviana Stănescu</td>\n",
              "      <td>092338961X</td>\n",
              "      <td>4.80</td>\n",
              "      <td>2003</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Meeting Eyes Bindery</td>\n",
              "      <td>5:4</td>\n",
              "      <td>...</td>\n",
              "      <td>3:0</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>66.0</td>\n",
              "      <td>Poetry. Translation. DIARY OF A CLONE is a sma...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Diary of a Clone Saviana Stănescu 2003 Meeting...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1850303</th>\n",
              "      <td>1499980</td>\n",
              "      <td>The O'Brien Book of Irish Fairy Tales &amp; Legends</td>\n",
              "      <td>Una  Leavy</td>\n",
              "      <td>0862784824</td>\n",
              "      <td>4.24</td>\n",
              "      <td>1996</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>O'Brien Press</td>\n",
              "      <td>5:39</td>\n",
              "      <td>...</td>\n",
              "      <td>3:8</td>\n",
              "      <td>2:4</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:95</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>96.0</td>\n",
              "      <td>Irish fairy tales and legends are full of ench...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The O'Brien Book of Irish Fairy Tales &amp; Legend...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1850305</th>\n",
              "      <td>1499988</td>\n",
              "      <td>Irish Folk and Fairy Tales Omnibus Edition</td>\n",
              "      <td>Michael Scott</td>\n",
              "      <td>0751508861</td>\n",
              "      <td>4.19</td>\n",
              "      <td>1989</td>\n",
              "      <td>24</td>\n",
              "      <td>8</td>\n",
              "      <td>Sphere</td>\n",
              "      <td>5:140</td>\n",
              "      <td>...</td>\n",
              "      <td>3:42</td>\n",
              "      <td>2:13</td>\n",
              "      <td>1:4</td>\n",
              "      <td>total:311</td>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>637.0</td>\n",
              "      <td>Here, collected in one volume, are tales and l...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Irish Folk and Fairy Tales Omnibus Edition Mic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1850306</th>\n",
              "      <td>1499990</td>\n",
              "      <td>Robin Hood: The Shaping of the Legend</td>\n",
              "      <td>Jeffrey L. Singman</td>\n",
              "      <td>0313301018</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1998</td>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>Praeger</td>\n",
              "      <td>5:0</td>\n",
              "      <td>...</td>\n",
              "      <td>3:1</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>224.0</td>\n",
              "      <td>Among the narrative traditions of the Middle A...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Robin Hood: The Shaping of the Legend Jeffrey ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1850308</th>\n",
              "      <td>1499992</td>\n",
              "      <td>Competing on Value</td>\n",
              "      <td>Mack Hanan</td>\n",
              "      <td>0814450369</td>\n",
              "      <td>3.50</td>\n",
              "      <td>1991</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "      <td>Amacom</td>\n",
              "      <td>5:2</td>\n",
              "      <td>...</td>\n",
              "      <td>3:2</td>\n",
              "      <td>2:2</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:8</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>220.0</td>\n",
              "      <td>Presents a new approach to selling that emphas...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Competing on Value Mack Hanan 1991 Amacom Pres...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1850309</th>\n",
              "      <td>1499999</td>\n",
              "      <td>Fantastic Antone Grows Up: Adolescents and Adu...</td>\n",
              "      <td>Judith Kleinfeld</td>\n",
              "      <td>1889963119</td>\n",
              "      <td>4.00</td>\n",
              "      <td>2000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>University of Alaska Press</td>\n",
              "      <td>5:4</td>\n",
              "      <td>...</td>\n",
              "      <td>3:4</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:13</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>425.0</td>\n",
              "      <td>&lt;i&gt;Fantastic Antone Grows Up&lt;/i&gt; is a field gu...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Fantastic Antone Grows Up: Adolescents and Adu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1163326 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Id                                               Name  \\\n",
              "0        1900511                                         Barbarossa   \n",
              "2        1900514                               Images of Barbarossa   \n",
              "3        1900520        Romania After 2000: Five New Romanian Plays   \n",
              "4        1900521           Global Foreigners: An Anthology of Plays   \n",
              "5        1900525                                   Diary of a Clone   \n",
              "...          ...                                                ...   \n",
              "1850303  1499980    The O'Brien Book of Irish Fairy Tales & Legends   \n",
              "1850305  1499988         Irish Folk and Fairy Tales Omnibus Edition   \n",
              "1850306  1499990              Robin Hood: The Shaping of the Legend   \n",
              "1850308  1499992                                 Competing on Value   \n",
              "1850309  1499999  Fantastic Antone Grows Up: Adolescents and Adu...   \n",
              "\n",
              "                        Authors        ISBN  Rating  PublishYear  \\\n",
              "0            Christopher Ailsby  1840138009    3.00         2007   \n",
              "2            Christopher Ailsby  0711028257    3.50         2001   \n",
              "3        Daniel Charles Gerould  0595436560    4.00         2007   \n",
              "4              Saviana Stănescu  1905422423    4.60         2006   \n",
              "5              Saviana Stănescu  092338961X    4.80         2003   \n",
              "...                         ...         ...     ...          ...   \n",
              "1850303              Una  Leavy  0862784824    4.24         1996   \n",
              "1850305           Michael Scott  0751508861    4.19         1989   \n",
              "1850306      Jeffrey L. Singman  0313301018    3.00         1998   \n",
              "1850308              Mack Hanan  0814450369    3.50         1991   \n",
              "1850309        Judith Kleinfeld  1889963119    4.00         2000   \n",
              "\n",
              "         PublishMonth  PublishDay  \\\n",
              "0                   4           1   \n",
              "2                   1          25   \n",
              "3                   9           1   \n",
              "4                  12           7   \n",
              "5                   1           1   \n",
              "...               ...         ...   \n",
              "1850303             9          10   \n",
              "1850305            24           8   \n",
              "1850306            23           7   \n",
              "1850308            22           4   \n",
              "1850309             1           1   \n",
              "\n",
              "                                           Publisher RatingDist5  ...  \\\n",
              "0                                     New Line Books         5:0  ...   \n",
              "2                                      Ian Allan Ltd         5:0  ...   \n",
              "3        Martin E. Segal Theatre Center Publications         5:1  ...   \n",
              "4                                      Seagull Books         5:4  ...   \n",
              "5                               Meeting Eyes Bindery         5:4  ...   \n",
              "...                                              ...         ...  ...   \n",
              "1850303                                O'Brien Press        5:39  ...   \n",
              "1850305                                       Sphere       5:140  ...   \n",
              "1850306                                      Praeger         5:0  ...   \n",
              "1850308                                       Amacom         5:2  ...   \n",
              "1850309                   University of Alaska Press         5:4  ...   \n",
              "\n",
              "        RatingDist3 RatingDist2 RatingDist1 RatingDistTotal CountsOfReview  \\\n",
              "0               3:1         2:0         1:0         total:1              0   \n",
              "2               3:2         2:1         1:0         total:8              0   \n",
              "3               3:1         2:0         1:0         total:6              0   \n",
              "4               3:1         2:0         1:0         total:5              0   \n",
              "5               3:0         2:0         1:0         total:5              0   \n",
              "...             ...         ...         ...             ...            ...   \n",
              "1850303         3:8         2:4         1:0        total:95              1   \n",
              "1850305        3:42        2:13         1:4       total:311             10   \n",
              "1850306         3:1         2:0         1:0         total:1              0   \n",
              "1850308         3:2         2:2         1:0         total:8              1   \n",
              "1850309         3:4         2:0         1:0        total:13              4   \n",
              "\n",
              "         Language PagesNumber  \\\n",
              "0             NaN       192.0   \n",
              "2             NaN       256.0   \n",
              "3             NaN       226.0   \n",
              "4             NaN       320.0   \n",
              "5             NaN        66.0   \n",
              "...           ...         ...   \n",
              "1850303       NaN        96.0   \n",
              "1850305       NaN       637.0   \n",
              "1850306       NaN       224.0   \n",
              "1850308       NaN       220.0   \n",
              "1850309       NaN       425.0   \n",
              "\n",
              "                                               Description  \\\n",
              "0        On 22 June 1941, Adolf Hitler launched Operati...   \n",
              "2        On 22 June 1941, Adolf Hitler launched Operati...   \n",
              "3        The first anthology of new Romanian Drama publ...   \n",
              "4        In Waxing West, Daniella, newly arrived in the...   \n",
              "5        Poetry. Translation. DIARY OF A CLONE is a sma...   \n",
              "...                                                    ...   \n",
              "1850303  Irish fairy tales and legends are full of ench...   \n",
              "1850305  Here, collected in one volume, are tales and l...   \n",
              "1850306  Among the narrative traditions of the Middle A...   \n",
              "1850308  Presents a new approach to selling that emphas...   \n",
              "1850309  <i>Fantastic Antone Grows Up</i> is a field gu...   \n",
              "\n",
              "        Count of text reviews  \\\n",
              "0                         NaN   \n",
              "2                         NaN   \n",
              "3                         NaN   \n",
              "4                         NaN   \n",
              "5                         NaN   \n",
              "...                       ...   \n",
              "1850303                   1.0   \n",
              "1850305                  10.0   \n",
              "1850306                   0.0   \n",
              "1850308                   1.0   \n",
              "1850309                   4.0   \n",
              "\n",
              "                                                   Content  \n",
              "0        Barbarossa Christopher Ailsby 2007 New Line Bo...  \n",
              "2        Images of Barbarossa Christopher Ailsby 2001 I...  \n",
              "3        Romania After 2000: Five New Romanian Plays Da...  \n",
              "4        Global Foreigners: An Anthology of Plays Savia...  \n",
              "5        Diary of a Clone Saviana Stănescu 2003 Meeting...  \n",
              "...                                                    ...  \n",
              "1850303  The O'Brien Book of Irish Fairy Tales & Legend...  \n",
              "1850305  Irish Folk and Fairy Tales Omnibus Edition Mic...  \n",
              "1850306  Robin Hood: The Shaping of the Legend Jeffrey ...  \n",
              "1850308  Competing on Value Mack Hanan 1991 Amacom Pres...  \n",
              "1850309  Fantastic Antone Grows Up: Adolescents and Adu...  \n",
              "\n",
              "[1163326 rows x 21 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Drop duplicates and missing values\n",
        "book_data = book_data.drop_duplicates()\n",
        "print(book_data.isna().sum())\n",
        "book_data.dropna(subset=['Publisher', 'Description'], inplace=True)\n",
        "book_data['PagesNumber'] = book_data['pagesNumber'].combine_first(book_data['PagesNumber'])\n",
        "book_data.drop(columns=['pagesNumber'], inplace=True)\n",
        "book_data['Content'] = book_data[['Name', 'Authors', 'PublishYear', 'Publisher', 'Description']].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
        "book_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocess Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We preprocess our content column using tokenization and word stemming. This ensures that we get the most information possible from the data and helps with the vectorization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LIM1gSaQpg_",
        "outputId": "6b86d3e0-fc91-4110-f7b3-12258851d768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1163326 entries, 0 to 1850309\n",
            "Data columns (total 21 columns):\n",
            " #   Column                 Non-Null Count    Dtype  \n",
            "---  ------                 --------------    -----  \n",
            " 0   Id                     1163326 non-null  int64  \n",
            " 1   Name                   1163326 non-null  object \n",
            " 2   Authors                1163326 non-null  object \n",
            " 3   ISBN                   1160256 non-null  object \n",
            " 4   Rating                 1163326 non-null  float64\n",
            " 5   PublishYear            1163326 non-null  int64  \n",
            " 6   PublishMonth           1163326 non-null  int64  \n",
            " 7   PublishDay             1163326 non-null  int64  \n",
            " 8   Publisher              1163326 non-null  object \n",
            " 9   RatingDist5            1163326 non-null  object \n",
            " 10  RatingDist4            1163326 non-null  object \n",
            " 11  RatingDist3            1163326 non-null  object \n",
            " 12  RatingDist2            1163326 non-null  object \n",
            " 13  RatingDist1            1163326 non-null  object \n",
            " 14  RatingDistTotal        1163326 non-null  object \n",
            " 15  CountsOfReview         1163326 non-null  int64  \n",
            " 16  Language               147265 non-null   object \n",
            " 17  PagesNumber            1163326 non-null  float64\n",
            " 18  Description            1163326 non-null  object \n",
            " 19  Count of text reviews  347959 non-null   float64\n",
            " 20  Content                1163326 non-null  object \n",
            "dtypes: float64(3), int64(5), object(13)\n",
            "memory usage: 195.3+ MB\n"
          ]
        }
      ],
      "source": [
        "book_data['Content'] = book_data['Content'].apply(preprocess_text)\n",
        "book_data.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vectorize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the vectorizer from the genre classifier and vectorize our content column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse import vstack\n",
        "\n",
        "def process_in_chunks(df, chunk_size=10000):\n",
        "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n",
        "    sparse_matrices = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        chunk = df.iloc[i*chunk_size : (i+1)*chunk_size]\n",
        "        sparse_matrix = vectorizer.transform(chunk['Content'])\n",
        "        sparse_matrices.append(sparse_matrix)\n",
        "\n",
        "    combined_sparse_matrix = vstack(sparse_matrices)\n",
        "    return combined_sparse_matrix\n",
        "\n",
        "book_vectors = process_in_chunks(book_data, chunk_size=10000)\n",
        "book_vectors = book_vectors.astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Genre Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use our genre classifier to assign each book with a genre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 32)                320032    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320490 (1.22 MB)\n",
            "Trainable params: 320426 (1.22 MB)\n",
            "Non-trainable params: 64 (256.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.load_model('./dense32model8972.h5')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36354/36354 [==============================] - 19s 510us/step\n"
          ]
        }
      ],
      "source": [
        "genre_pred = model.predict(book_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Name</th>\n",
              "      <th>Authors</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Rating</th>\n",
              "      <th>PublishYear</th>\n",
              "      <th>PublishMonth</th>\n",
              "      <th>PublishDay</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>RatingDist5</th>\n",
              "      <th>...</th>\n",
              "      <th>RatingDist2</th>\n",
              "      <th>RatingDist1</th>\n",
              "      <th>RatingDistTotal</th>\n",
              "      <th>CountsOfReview</th>\n",
              "      <th>Language</th>\n",
              "      <th>PagesNumber</th>\n",
              "      <th>Description</th>\n",
              "      <th>Count of text reviews</th>\n",
              "      <th>Content</th>\n",
              "      <th>Genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1900511</td>\n",
              "      <td>Barbarossa</td>\n",
              "      <td>Christopher Ailsby</td>\n",
              "      <td>1840138009</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2007</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>New Line Books</td>\n",
              "      <td>5:0</td>\n",
              "      <td>...</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>192.0</td>\n",
              "      <td>On 22 June 1941, Adolf Hitler launched Operati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>barbarossa christoph ailsbi 2007 new line book...</td>\n",
              "      <td>fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1900514</td>\n",
              "      <td>Images of Barbarossa</td>\n",
              "      <td>Christopher Ailsby</td>\n",
              "      <td>0711028257</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2001</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>Ian Allan Ltd</td>\n",
              "      <td>5:0</td>\n",
              "      <td>...</td>\n",
              "      <td>2:1</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:8</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>256.0</td>\n",
              "      <td>On 22 June 1941, Adolf Hitler launched Operati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>imag of barbarossa christoph ailsbi 2001 ian a...</td>\n",
              "      <td>fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1900520</td>\n",
              "      <td>Romania After 2000: Five New Romanian Plays</td>\n",
              "      <td>Daniel Charles Gerould</td>\n",
              "      <td>0595436560</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2007</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>Martin E. Segal Theatre Center Publications</td>\n",
              "      <td>5:1</td>\n",
              "      <td>...</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:6</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>226.0</td>\n",
              "      <td>The first anthology of new Romanian Drama publ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>romania after 2000 : five new romanian play da...</td>\n",
              "      <td>sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1900521</td>\n",
              "      <td>Global Foreigners: An Anthology of Plays</td>\n",
              "      <td>Saviana Stănescu</td>\n",
              "      <td>1905422423</td>\n",
              "      <td>4.6</td>\n",
              "      <td>2006</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>Seagull Books</td>\n",
              "      <td>5:4</td>\n",
              "      <td>...</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>320.0</td>\n",
              "      <td>In Waxing West, Daniella, newly arrived in the...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>global foreign : an antholog of play saviana s...</td>\n",
              "      <td>psychology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1900525</td>\n",
              "      <td>Diary of a Clone</td>\n",
              "      <td>Saviana Stănescu</td>\n",
              "      <td>092338961X</td>\n",
              "      <td>4.8</td>\n",
              "      <td>2003</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Meeting Eyes Bindery</td>\n",
              "      <td>5:4</td>\n",
              "      <td>...</td>\n",
              "      <td>2:0</td>\n",
              "      <td>1:0</td>\n",
              "      <td>total:5</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>66.0</td>\n",
              "      <td>Poetry. Translation. DIARY OF A CLONE is a sma...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>diari of a clone saviana stănescu 2003 meet ey...</td>\n",
              "      <td>fantasy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Id                                         Name  \\\n",
              "0  1900511                                   Barbarossa   \n",
              "2  1900514                         Images of Barbarossa   \n",
              "3  1900520  Romania After 2000: Five New Romanian Plays   \n",
              "4  1900521     Global Foreigners: An Anthology of Plays   \n",
              "5  1900525                             Diary of a Clone   \n",
              "\n",
              "                  Authors        ISBN  Rating  PublishYear  PublishMonth  \\\n",
              "0      Christopher Ailsby  1840138009     3.0         2007             4   \n",
              "2      Christopher Ailsby  0711028257     3.5         2001             1   \n",
              "3  Daniel Charles Gerould  0595436560     4.0         2007             9   \n",
              "4        Saviana Stănescu  1905422423     4.6         2006            12   \n",
              "5        Saviana Stănescu  092338961X     4.8         2003             1   \n",
              "\n",
              "   PublishDay                                    Publisher RatingDist5  ...  \\\n",
              "0           1                               New Line Books         5:0  ...   \n",
              "2          25                                Ian Allan Ltd         5:0  ...   \n",
              "3           1  Martin E. Segal Theatre Center Publications         5:1  ...   \n",
              "4           7                                Seagull Books         5:4  ...   \n",
              "5           1                         Meeting Eyes Bindery         5:4  ...   \n",
              "\n",
              "  RatingDist2 RatingDist1 RatingDistTotal CountsOfReview Language  \\\n",
              "0         2:0         1:0         total:1              0      NaN   \n",
              "2         2:1         1:0         total:8              0      NaN   \n",
              "3         2:0         1:0         total:6              0      NaN   \n",
              "4         2:0         1:0         total:5              0      NaN   \n",
              "5         2:0         1:0         total:5              0      NaN   \n",
              "\n",
              "   PagesNumber                                        Description  \\\n",
              "0        192.0  On 22 June 1941, Adolf Hitler launched Operati...   \n",
              "2        256.0  On 22 June 1941, Adolf Hitler launched Operati...   \n",
              "3        226.0  The first anthology of new Romanian Drama publ...   \n",
              "4        320.0  In Waxing West, Daniella, newly arrived in the...   \n",
              "5         66.0  Poetry. Translation. DIARY OF A CLONE is a sma...   \n",
              "\n",
              "   Count of text reviews                                            Content  \\\n",
              "0                    NaN  barbarossa christoph ailsbi 2007 new line book...   \n",
              "2                    NaN  imag of barbarossa christoph ailsbi 2001 ian a...   \n",
              "3                    NaN  romania after 2000 : five new romanian play da...   \n",
              "4                    NaN  global foreign : an antholog of play saviana s...   \n",
              "5                    NaN  diari of a clone saviana stănescu 2003 meet ey...   \n",
              "\n",
              "        Genre  \n",
              "0     fantasy  \n",
              "2     fantasy  \n",
              "3      sports  \n",
              "4  psychology  \n",
              "5     fantasy  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "genres = ['fantasy', 'science', 'crime', 'history', 'horror', 'thriller', 'psychology', 'romance', 'sports', 'travel']\n",
        "genre_pred.shape\n",
        "predicted_classes = np.argmax(genre_pred, axis=1)\n",
        "predicted_genres = [genres[class_idx] for class_idx in predicted_classes]\n",
        "book_data['Genre'] = predicted_genres\n",
        "book_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combined Data (Finding the Overlap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we merge both the ratings and books dataframes on their 'Name' column (i.e., the name of the book). This is required as the original rating data is only connected to book names, but we also want to connect them to book ID's so that we can build one user profile that can be used for evaluation by both the content-based and collaborative filtering models. \n",
        "\n",
        "Initially, we attempted to merge the data directly, but there was a key issue with our approach. A user with userID = 5 had rated a book called 'The Secret Garden' in the ratings dataframe. However, multiple books shared this book name and each book had a seperate ID. Thus, this rating of 'The Secret Garden' was copied to each unique book that shared this name. Furthermore, we were unable to know which of these BookID's refer to the actual book that user 5 rated. Thus, for the merged dataframe we decided to remove all instances of ratings where the book Name is not unique in the books dataframe. To do this, we must ensure that each 'UserID' and book 'Name' pair is associated with only a single unique 'bookID'. To ensure that there are no cases where a (UserID, Name) pair is associated with multiple BookID's, we need to first remove all book names that aren't unique in the original books dataframe prior to merging, as seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "187248 rows were removed from the books dataframe.\n",
            "(976078, 22)\n"
          ]
        }
      ],
      "source": [
        "# For each book name, get a count for the number of times it appears in the book_data.\n",
        "book_name_counts = book_data['Name'].value_counts()\n",
        "# Seperate the unique book names.\n",
        "unique_names = book_name_counts[book_name_counts == 1].index\n",
        "\n",
        "# Make a copy of the dataframe which retains ONLY the books with unique names.\n",
        "book_df = book_data[book_data['Name'].isin(unique_names)].copy()\n",
        "\n",
        "# Print the number of entries that were removed from the original books dataframe.\n",
        "print(f\"{book_data.shape[0] - book_df.shape[0]} rows were removed from the books dataframe.\")\n",
        "print(book_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since both the 'rating_df' and 'book_df' dataframes have a 'Rating' value, we must drop one of them. We have chosen to drop the overall 'Rating' of a book, as it is less helpful when creating user profiles and can still be easily retrieved from the original dataframe using the BookID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop the rating column from book_df.\n",
        "book_df.drop('Rating', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we merge the two dataframes and also drop all unecessary columns so that we can see a clear connection between a user rating and the book ID of the rated book. We also rename the 'Id' column from the books dataframe to 'BookID, and the 'ID' column from the ratings dataframe to 'UserID' to increase clarity.\n",
        "\n",
        "Lastly, we print the head of the data, aswell as the shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   UserID                                   Name Rating   BookID        ISBN\n",
            "0       5  The Crystal Cave (Arthurian Saga, #1)      4  1293575  0449206440\n",
            "1     139  The Crystal Cave (Arthurian Saga, #1)      4  1293575  0449206440\n",
            "2     338  The Crystal Cave (Arthurian Saga, #1)      4  1293575  0449206440\n",
            "3     853  The Crystal Cave (Arthurian Saga, #1)      5  1293575  0449206440\n",
            "4    7268  The Crystal Cave (Arthurian Saga, #1)      4  1293575  0449206440\n",
            "(56302, 5)\n"
          ]
        }
      ],
      "source": [
        "merged_df = pd.merge(rating_df, book_df, on='Name', how='inner')\n",
        "\n",
        "colsToDrop = ['Authors', 'PublishYear', 'PublishMonth', 'PublishDay', 'Publisher', 'RatingDist5', 'RatingDist4', \n",
        "              'RatingDist3', 'RatingDist2', 'RatingDist1', 'RatingDistTotal', 'CountsOfReview', 'Language',\n",
        "              'Content', 'Description', 'Count of text reviews', 'PagesNumber', 'Genre']\n",
        "\n",
        "merged_df.drop(colsToDrop, axis=1, inplace=True)\n",
        "\n",
        "merged_df.rename(columns={'Id':'BookID', 'ID':'UserID'}, inplace=True)\n",
        "\n",
        "print(merged_df.head())\n",
        "print(merged_df.shape)\n",
        "# print(f\"There are {merged_df['BookID'].nunique()} books that are comming in both dataframes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finding Users to Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will use the merged dataframe to find 3 users that will be used to evaluate the performance of our models. The goal is to find users who have enough ratings to provide meaningful information about their likes & dislikes, while ensuring that there is a reasonable split between liked and disliked books. For likes, we will set the threshold at 4 or more stars, i.e., a user 'likes' a book if they have rated it at 4 or above.\n",
        "\n",
        "To obtain meaninful information, we will only evaluate users that 'like' (rating >= 4) less than half of the total books that they've rated. This ensures that there is a useful amount of information on both the user's likes and dislikes. However, we will also see how the model performs for users with different rating habits. In particular, we will see how the model performs for three specific users:\n",
        "- 20+ ratings and 10 liked books\n",
        "- 80+ ratings and 40 liked books\n",
        "- 326+ ratings and 163 liked books\n",
        "\n",
        "We aim to see the value provide by the three models for the three chosen users with respect to the number of books that they have indicated as a 'like', as well as the total amount of feedback that they have provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following users will be evaluated: \n",
            "\n",
            "User 63, who liked 10 books and rated 27 in total.\n",
            "User 3422, who liked 40 books and rated 99 in total.\n",
            "User 3259, who liked 163 books and rated 358 in total.\n"
          ]
        }
      ],
      "source": [
        "# Get a count of the number of books that each user has rated.\n",
        "ratings_count = merged_df.groupby('UserID').size()\n",
        "\n",
        "# Get a dataframe that only contains books that users like (i.e., rating at 4 or above)\n",
        "likes_df = merged_df[merged_df['Rating'] >= 4]\n",
        "# Get a count of the number of books that each user 'likes'. \n",
        "likes_count = likes_df.groupby('UserID').size()\n",
        "\n",
        "# Find users with three differing numbers of liked books.\n",
        "users_with_10_likes = likes_count[likes_count == 10].index\n",
        "users_with_40_likes = likes_count[likes_count == 40].index\n",
        "users_with_160_likes = likes_count[likes_count >= 160].index\n",
        "\n",
        "# Initialise a list to hold the ID's of the users that are chosen for evaluation.\n",
        "users_to_evaluate = []\n",
        "\n",
        "# Out of all users with 10 liked books,\n",
        "for user_id in users_with_10_likes:\n",
        "    # If they have rated more than 20 books,\n",
        "    if (ratings_count[user_id] > 2*likes_count[user_id]):\n",
        "        # Add them to the users_to_evaluate list, and break (as we need only one user of this type).\n",
        "        users_to_evaluate.append(user_id)\n",
        "        break\n",
        "\n",
        "# Out of all users with 40 liked books,\n",
        "for user_id in users_with_40_likes:\n",
        "    # If they have rated more than 80 books,\n",
        "    if (ratings_count[user_id] > 2*likes_count[user_id]):\n",
        "        # Add them to the users_to_evaluate list, and break (as we need only one user of this type).\n",
        "        users_to_evaluate.append(user_id)\n",
        "        break\n",
        "\n",
        "# Out of all users with >= 160 liked books,\n",
        "for user_id in users_with_160_likes:\n",
        "    # If they have rated more than double their liked amount,\n",
        "    if (ratings_count[user_id] > 2*likes_count[user_id]):\n",
        "        # Add them to the users_to_evaluate list, and break (as we need only one user of this type).\n",
        "        users_to_evaluate.append(user_id)\n",
        "        break\n",
        "\n",
        "# Print information about the chosen users.\n",
        "print(\"The following users will be evaluated: \\n\")\n",
        "for user in users_to_evaluate:\n",
        "    print(f\"User {user}, who liked {likes_count[user]} books and rated {ratings_count[user]} in total.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# User Profiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the cold start problem we do not have any user ratings to form a user profile. Instead we must use the user preferences to form a user profile. We collect the user preferences from our interface and then select the highest rated books that match each of the genres and authors that the user prefers. We filter these results by their ideal book length (within 100 pages). We then combine all of the content from those books into 1 document and vectorize it, this is the user profile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_user_profile(user_pref, df, vectorizer):\n",
        "    matches_dfs = []\n",
        "    for genre in user_pref['Genres']:\n",
        "        genre_matches = df[df['Genre'] == genre]\n",
        "        genre_matches = genre_matches[abs(genre_matches['PagesNumber'] - user_pref['Length']) <= 100].sort_values(by=['Rating'], ascending=False).head(10)\n",
        "        matches_dfs.append(genre_matches)\n",
        "    for author in user_pref['Authors']:\n",
        "        author_matches = df[df['Authors'].str.contains(author, case=False, na=False)]\n",
        "        author_matches = author_matches[abs(author_matches['PagesNumber'] - user_pref['Length']) <= 100].sort_values(by=['Rating'], ascending=False).head(10)\n",
        "        matches_dfs.append(author_matches)\n",
        "    matches = pd.concat(matches_dfs, ignore_index=True)\n",
        "    # Drop duplicate rows if necessary\n",
        "    matches = matches.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    document = ' '.join(matches['Content'])\n",
        "    return vectorizer.transform([document]).toarray()\n",
        "\n",
        "kid_pref = {\n",
        "    'Genres': ['fantasy', 'sports', 'science'],\n",
        "    'Authors': ['J.K Rowling', 'Enid Blyton', 'Suzanne Collins', 'Andy Griffiths', 'Roald Dahl', 'Jeff Kinney'],\n",
        "    'Length': 100\n",
        "}\n",
        "ya_pref = {\n",
        "    'Genres': ['fantasy', 'romance', 'thriller'],\n",
        "    'Authors': ['J.K Rowling', 'Suzanne Collins', 'John Green', 'Veronica Roth', 'James Dashner', 'Karen M. McManus'],\n",
        "    'Length': 300\n",
        "}\n",
        "crime_pref = {\n",
        "    'Genres': ['crime', 'thriller', 'horror', 'romance', 'psychology'],\n",
        "    'Authors': ['Sarah J. Maas', 'Agatha Christie', 'Karen M. McManus', 'Ian Rankin', 'Lee Child', 'Richard Osman', 'Kate Atkinson'],\n",
        "    'Length': 500\n",
        "}\n",
        "classics_pref = {\n",
        "    'Genres': ['fantasy', 'history', 'romance', 'thriller'],\n",
        "    'Authors': ['Agatha Christie', 'Enid Blyton', 'Charles Dickens', 'Roald Dahl', 'Jane Austen', 'William Shakespeare', 'J.D. Salinger', 'George Orwell'],\n",
        "    'Length': 350\n",
        "}\n",
        "nonfiction_pref = {\n",
        "    'Genres': ['science', 'history', 'psychology', 'sports', 'travel'],\n",
        "    'Authors': ['Tom Holland', 'David McCullough', 'William L. Shirer', 'A.J. Liebling', 'David Attenborough'],\n",
        "    'Length': 500\n",
        "}\n",
        "kid_profile = generate_user_profile(kid_pref, book_data, vectorizer)\n",
        "ya_profile = generate_user_profile(ya_pref, book_data, vectorizer)\n",
        "crime_profile = generate_user_profile(crime_pref, book_data, vectorizer)\n",
        "classics_profile = generate_user_profile(classics_pref, book_data, vectorizer)\n",
        "nonfiction_profile = generate_user_profile(nonfiction_pref, book_data, vectorizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have ratings data we can use the user's 'liked' books (rating > 4) as their user profile. This allows us to evaluate the model. We take the 3 users chosen earlier and create user profiles for them using half of their liked books, combining the content columns into one document and then vectorizing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def user_profile_from_liked_books(user_id, likes_df, book_data, vectorizer):\n",
        "    all_liked = likes_df[likes_df['UserID'] == user_id].copy()\n",
        "    train_liked, test_liked = train_test_split(all_liked, test_size=0.5, random_state=42)\n",
        "    all_content = []\n",
        "    for bookID in train_liked['BookID']:\n",
        "        all_content.append(book_data[book_data['Id'] == bookID]['Content'].iloc[0])\n",
        "    document = ' '.join(all_content)\n",
        "    \n",
        "    return vectorizer.transform([document]).toarray(), train_liked, test_liked\n",
        "\n",
        "user_profiles = {}\n",
        "test_sets = {}\n",
        "train_sets = {}\n",
        "for user in users_to_evaluate:\n",
        "    user_profile, train_liked, test_liked = user_profile_from_liked_books(user, likes_df, book_data, vectorizer)\n",
        "    user_profiles[user] = user_profile\n",
        "    test_sets[user] = test_liked\n",
        "    train_sets[user] = train_liked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then compute the Jaccard similarity between our user profile and any book vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jaccard_similarity(vec1, vec2):\n",
        "    intersection = np.sum(np.minimum(vec1, vec2))\n",
        "    union = np.sum(np.maximum(vec1, vec2))\n",
        "    return intersection / union if union != 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the top n recommendations by going through every book and computing the Jaccard similarity. We ignore the books used in the user profile and we ignore books with equal or less reviews than our threshold. We multilply the similarity by the similarity weight and add the normalized rating by (1 - similarity weight). This ensures that we are recommending books that other users have read and liked. We sort the books by similarity and take the top N."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def get_top_n(book_data, book_vectors, user_profile, train_liked, threshold, n, similarity_weight):\n",
        "    max_review_count = max(book_data['CountsOfReview'])\n",
        "    similarities = {}\n",
        "    for index, vector in enumerate(book_vectors):\n",
        "        bookID = book_data['Id'].iloc[index]\n",
        "        if bookID in train_liked['BookID'].values:\n",
        "            continue\n",
        "        review_count = book_data['CountsOfReview'].iloc[index]\n",
        "        rating = book_data['Rating'].iloc[index]\n",
        "        if review_count <= threshold:\n",
        "            continue\n",
        "        similarity = jaccard_similarity(user_profile, vector.toarray())\n",
        "        normalized_reviews = (review_count)/(max_review_count)\n",
        "        normalized_rating = (rating*normalized_reviews)/5\n",
        "        # weight the similarity by their review count\n",
        "        similarities[index] = (similarity_weight * similarity) + ((1-similarity_weight) * normalized_rating)\n",
        "        # similarities[index] = similarity\n",
        "\n",
        "    sorted_books = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:n]\n",
        "    sorted_book_names = []\n",
        "    for book in sorted_books:\n",
        "        name = book_data.iloc[book[0]]['Name']\n",
        "        bookID = book_data.iloc[book[0]]['Id']\n",
        "        sorted_book_names.append({\"Name\": name, \"BookID\": bookID})\n",
        "    return sorted_book_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then evaluate our model on precision, recall and f1_score. We compute TP, FP, TN and FN by checking if our recommendations are in the users 'test' set that we kept out of their profile. If we recommend a book to the user and they like it then that is a TP. If they don't like a recommendation then that is a FP. If we don't recommend a book to the user and they don't like it then that is a TN. If we don't recommend a book to a user but they do like it then that is a FN.\n",
        "\n",
        "We assume that if a user has not rated a book that they don't like it since we have no idea about their preferences. However, in reality they would like some of those books. This means that our TN value will be very high as almost all of the books we don't recommend will be TN. Therefore, accuracy is not a useful metric to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_recommendations(top_n, n, test_liked, total_books):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    for book in top_n:\n",
        "        if book['BookID'] in test_liked['BookID'].values:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FP += 1\n",
        "    FN = len(test_liked) - TP\n",
        "    TN = total_books - n - FP\n",
        "    accuracy = (TP + TN) / (total_books) if (total_books) != 0 else 0\n",
        "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    return accuracy, precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We evaluate the model for each user with the different parameters to determine the best ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For user 63\n",
            "With n = 10 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 10 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 10 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 10 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 10 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 10 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 10 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 10 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 10 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828078406562\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 30 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999484235219686\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 63\n",
            "With n = 50 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999140392032809\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 10 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999828076189756\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 10 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999828076189756\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 10 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828076189756\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 10 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999828076189756\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 10 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999828076189756\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 10 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828076189756\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 10 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999845268570781\n",
            "Precision: 0.1\n",
            "Recall: 0.05\n",
            "f1_score: 0.06666666666666667\n",
            "For user 3422\n",
            "With n = 10 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999845268570781\n",
            "Precision: 0.1\n",
            "Recall: 0.05\n",
            "f1_score: 0.06666666666666667\n",
            "For user 3422\n",
            "With n = 10 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828076189756\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 30 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999484228569268\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 30 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999484228569268\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 30 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999484228569268\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 30 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999501420950292\n",
            "Precision: 0.03333333333333333\n",
            "Recall: 0.05\n",
            "f1_score: 0.04\n",
            "For user 3422\n",
            "With n = 30 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999501420950292\n",
            "Precision: 0.03333333333333333\n",
            "Recall: 0.05\n",
            "f1_score: 0.04\n",
            "For user 3422\n",
            "With n = 30 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999501420950292\n",
            "Precision: 0.03333333333333333\n",
            "Recall: 0.05\n",
            "f1_score: 0.04\n",
            "For user 3422\n",
            "With n = 30 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999501420950292\n",
            "Precision: 0.03333333333333333\n",
            "Recall: 0.05\n",
            "f1_score: 0.04\n",
            "For user 3422\n",
            "With n = 30 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999501420950292\n",
            "Precision: 0.03333333333333333\n",
            "Recall: 0.05\n",
            "f1_score: 0.04\n",
            "For user 3422\n",
            "With n = 30 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999501420950292\n",
            "Precision: 0.03333333333333333\n",
            "Recall: 0.05\n",
            "f1_score: 0.04\n",
            "For user 3422\n",
            "With n = 50 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999140380948779\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 50 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999140380948779\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 50 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999140380948779\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3422\n",
            "With n = 50 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999157573329803\n",
            "Precision: 0.02\n",
            "Recall: 0.05\n",
            "f1_score: 0.02857142857142857\n",
            "For user 3422\n",
            "With n = 50 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999157573329803\n",
            "Precision: 0.02\n",
            "Recall: 0.05\n",
            "f1_score: 0.02857142857142857\n",
            "For user 3422\n",
            "With n = 50 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999157573329803\n",
            "Precision: 0.02\n",
            "Recall: 0.05\n",
            "f1_score: 0.02857142857142857\n",
            "For user 3422\n",
            "With n = 50 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999157573329803\n",
            "Precision: 0.02\n",
            "Recall: 0.05\n",
            "f1_score: 0.02857142857142857\n",
            "For user 3422\n",
            "With n = 50 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999157573329803\n",
            "Precision: 0.02\n",
            "Recall: 0.05\n",
            "f1_score: 0.02857142857142857\n",
            "For user 3422\n",
            "With n = 50 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999157573329803\n",
            "Precision: 0.02\n",
            "Recall: 0.05\n",
            "f1_score: 0.02857142857142857\n",
            "For user 3259\n",
            "With n = 10 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 10 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 10 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 10 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 10 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 10 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 10 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 10 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 10 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999828067174155\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 30 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999484201522465\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 0 and similarity weight = 1\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 0 and similarity weight = 0.9\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 0 and similarity weight = 0.8\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 500 and similarity weight = 1\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 500 and similarity weight = 0.9\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 500 and similarity weight = 0.8\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 1000 and similarity weight = 1\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 1000 and similarity weight = 0.9\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n",
            "For user 3259\n",
            "With n = 50 and threshold = 1000 and similarity weight = 0.8\n",
            "Accuracy: 0.9999140335870775\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "f1_score: 0\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "for user in users_to_evaluate:\n",
        "    ns = [10, 30, 50]\n",
        "    thresholds = [0, 500, 1000]\n",
        "    weights = [1, 0.9, 0.8]\n",
        "    for params in itertools.product(ns, thresholds, weights):\n",
        "        top_n = get_top_n(book_data, book_vectors, user_profiles[user], train_sets[user], params[1], params[0], params[2])\n",
        "        accuracy, precision, recall, f1_score = evaluate_recommendations(top_n, params[0], test_sets[user], len(book_data)-len(train_sets[user]))\n",
        "        print(\"For user\", user)\n",
        "        print(\"With n =\", params[0], \"and threshold =\", params[1], \"and similarity weight =\", params[2])\n",
        "        print(\"Accuracy:\", accuracy)\n",
        "        print(\"Precision:\", precision)\n",
        "        print(\"Recall:\", recall)\n",
        "        print(\"f1_score:\", f1_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The paramater values that produce the best precision, recall and f1 score are:\n",
        "N = 50, threshold = 1000 and similarity weight = 0.9.\n",
        "\n",
        "We ignore accuracy since the true negative value is very high so it makes accuracy an irrelevant metric. The actual precision, recall and f1 scores are very low. This is becuase we have over 1 million books in the dataset and only 5-81 liked books in each users test set. This means that even recommending 1 book that the user likes is a significant achievement because we can't know if the user likes books that they haven't rated, however it is fair to assume that they would like some of the recommendations (had they rated it) if we recommend at least 1 that they already like.\n",
        "\n",
        "We now do a final evaluation of the model with the chosen paramaters and average the results from our 3 users. We use a higher N (N = 100) since we want to see the performance of the model, a higher N will improve the chances of recommending a book that the user likes but may reduce precision and recall slightly if we do recommend a book that they like. This is ok for the evaluation step, as it allows us to see if the recommender is recommending any books that the user has already liked. In the actual implementation of the model we will choose a smaller N as we need to take into account the usability for the user, 100 recommendations is not user-friendly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall:\n",
            "Accuracy: 0.9998292201122778\n",
            "Precision: 0.006666666666666667\n",
            "Recall: 0.020731707317073172\n",
            "f1_score: 0.009218559218559219\n"
          ]
        }
      ],
      "source": [
        "sum_accuracy = 0\n",
        "sum_precision = 0\n",
        "sum_recall = 0\n",
        "sum_f1 = 0\n",
        "for user in users_to_evaluate:\n",
        "    n = 100\n",
        "    threshold = 1000\n",
        "    similarity_weight = 0.9\n",
        "    top_n = get_top_n(book_data, book_vectors, user_profiles[user], train_sets[user], threshold, n, similarity_weight)\n",
        "    accuracy, precision, recall, f1_score = evaluate_recommendations(top_n, n, test_sets[user], len(book_data)-len(train_sets[user]))\n",
        "    sum_accuracy += accuracy\n",
        "    sum_precision += precision\n",
        "    sum_recall += recall\n",
        "    sum_f1 += f1_score\n",
        "print(\"Overall:\")\n",
        "print(\"Accuracy:\", sum_accuracy/len(users_to_evaluate))\n",
        "print(\"Precision:\", sum_precision/len(users_to_evaluate))\n",
        "print(\"Recall:\", sum_recall/len(users_to_evaluate))\n",
        "print(\"f1_score:\", sum_f1/len(users_to_evaluate))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
